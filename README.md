# ðŸ§  RAG System with Qdrant + Ollama (Mistral)

This project implements a modular Retrieval-Augmented Generation (RAG) pipeline.

---

## ðŸ”§ Components

### `index_articles.py` â€” Indexing
- Cleans & chunks articles
- Generates embeddings
- Stores in Qdrant DB

### `query_rag.py` â€” Retrieval
- Embeds user query
- Searches Qdrant for relevant chunks
- Calls LLM with retrieved context

## ðŸ§ª Run

### 1. Index articles

```bash
python index_articles.py
```

### 2. Ask a question

```bash
python query_rag.py
```

---

## ðŸ“¦ Structure

```
rag_project/
â”œâ”€â”€ docs/
â”œâ”€â”€ index_articles.py
â”œâ”€â”€ query_rag.py
â”œâ”€â”€ utils.py
â”œâ”€â”€ config.py
â””â”€â”€ README.md
```

## ðŸ“Š Workflow Diagram

```mermaid
graph TD
    A[HTML/MDX Files] -->|Extract Metadata| B[Clean + Chunk]
    B -->|Process| C[Generate Embeddings]
    C -->|Store| D[Qdrant Database]
    E[User Prompt] -->|Embed| F[Search Query]
    F -->|Find| G[Retrieve Top K]
    G -->|Context| H[Generate Answer]
    H -->|Return| I[Final Response]
```

## ðŸ”§ Configuration

The system supports multiple models through `config.py`:

- **llama3.2**: 3072-dimensional vectors
- **mistral**: 4096-dimensional vectors

To switch models, update `CURRENT_MODEL` in `config.py`.
